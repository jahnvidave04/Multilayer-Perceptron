{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Evaluated based on : Implemented Backpropagation Algorithm, Multi-layer Perceptron,Testing accuracy.\n",
        "================================================================================\n",
        "Task Which we need to perform:\n",
        "1. Take two files as an input. We have to take input as numpy array because we have to produce an output as numpy array.\n",
        "2. Divide it into Training and Validation(used to make sure, model did not overfit).\n",
        "3. Provide our model which will be tested with an unseen test set.\n",
        "4. Use\n",
        "           1 Input Layer\n",
        "           1 Hidden Layer\n",
        "           1 Output Layer\n",
        "5. Labels are one hot encoded.\n",
        "6. Make sure to use appropriate activation function in Output.\n",
        "7. Free to use any no, of nodes in hidden layer.\n",
        "8. Provide one single function to see the n/w to predict the test set.\n",
        "    This function should o/p the labels one-hot encoded in numpy array."
      ],
      "metadata": {
        "id": "F9XLm-u_xs9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# genfromtxt function reads information from a text file and converts it into a NumPy array to perform 1st and 8th tasks.\n",
        "from numpy import genfromtxt\n",
        "# sets the seed value for the random number generator.\n",
        "# To ensure reproducibility that on every run the random value will be the same.\n",
        "np.random.seed(0)\n",
        "# computes the exponential value of a number\n",
        "from math import exp\n",
        "# Used to segregate the data in Training and Validation as stated in 2nd Task above.\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "v6E-65cpxjn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqG_B17CbJtU",
        "outputId": "0245b264-502d-4a99-fff8-7179b38ee8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 [Take two files as an input. We have to take input as numpy array because\n",
        "#         we have to produce an output as numpy array.]\n",
        "# =================================================================================\n",
        "# Fetching the data from given files and store it to x and y in numpy array Format.\n",
        "x_data = genfromtxt(\"/content/drive/My Drive/train_data.csv\", delimiter=',')\n",
        "y_labels = genfromtxt(\"/content/drive/My Drive/train_labels.csv\", delimiter=',')"
      ],
      "metadata": {
        "id": "3r0QdeJ-2YUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 [Divide it into Training and Validation(used to make sure, model did not overfit).]\n",
        "# =================================================================================\n",
        "# Let us split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_data, y_labels, test_size = 0.2)"
      ],
      "metadata": {
        "id": "I9j0eISv5BKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying the number of epochs, intermediate nodes and learning rate for the network\n",
        "# We are specifying this here because we have to use the values of these in future reference code.\n",
        "number_of_epoch  = 100\n",
        "features = 784\n",
        "samples = 10\n",
        "hidden_layer_nodes=20\n",
        "intermediate_nodes = 35\n",
        "number_of_classes = 4\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "o7d5iH4xJn7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid Activation Function\n",
        "# ===========================================================\n",
        "# Function for specifying Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Function for specifying Derivative of Sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "metadata": {
        "id": "sxhCXDwTJtDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "# ===========================================================\n",
        "# Function for specifying Softmax Activation Function\n",
        "def softmax(x):\n",
        "    value = np.exp(x) / np.exp(x).sum(axis=1, keepdims=True)\n",
        "    return value\n",
        "\n",
        "# Function for specifying Derivative of SoftMax function\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1 - softmax(x))"
      ],
      "metadata": {
        "id": "86_4d5pLZWyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the cross entropy\n",
        "def cross_entropy(actual_value,predicted_value):\n",
        "    mean_square_error = np.square(actual_value - predicted_value)\n",
        "    value = np.mean(mean_square_error)\n",
        "    return value"
      ],
      "metadata": {
        "id": "i4HS15naZZOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predicted data into one hot encoding\n",
        "def one_hot_encoding(x):\n",
        "    for index in range(0,len(x)):\n",
        "        x[index,x[index,:].argmax()]=1\n",
        "    value = (x == 1).astype(float)\n",
        "    return value"
      ],
      "metadata": {
        "id": "jKs_fYYUJxUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the accuracy of the model\n",
        "#=========================================================================\n",
        "def accuracy(y_true, y_pred):\n",
        "    if not (len(y_true) == len(y_pred)):\n",
        "        print('size(predicted_labels) != size(true_labels)')\n",
        "        return 0.0\n",
        "    correlation = 0\n",
        "    for index in range(0,len(y_true)):\n",
        "        correlation += 1 if (y_true[index] == y_pred[index]).all() else 0\n",
        "    return correlation/len(y_true)"
      ],
      "metadata": {
        "id": "oVCCPbaIZzaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed-forward propagation\n",
        "#================================================================================================================================\n",
        "def forward_propagation(input_data, weight_of_hidden_layer, bias_of_hidden_layer, weight_of_output_layer, bias_of_output_layer):\n",
        "    hidden_network = np.dot(input_data, weight_of_hidden_layer) + bias_of_hidden_layer\n",
        "    actual_hidden = sigmoid(hidden_network)\n",
        "    output_network = np.dot(actual_hidden, weight_of_output_layer) + bias_of_output_layer\n",
        "    actual_output = softmax(output_network)\n",
        "    return actual_output, actual_hidden, hidden_network"
      ],
      "metadata": {
        "id": "jLBrbqNseNYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation Algorithm\n",
        "def back_propagation(X_train, y_train, hidden_network, actual_hidden, weight_output, actual_output):\n",
        "    network_hidden_id = actual_output - y_train\n",
        "    grad_bias_output = network_hidden_id\n",
        "    grad_weight_output = np.dot(actual_hidden.T, network_hidden_id)\n",
        "    actual_hidden_id = np.dot(network_hidden_id, weight_output.T)\n",
        "    grad_weight_hidden = np.dot(X_train.T, sigmoid_derivative(hidden_network) * actual_hidden_id)\n",
        "    grad_bias_hidden = actual_hidden_id * sigmoid_derivative(hidden_network)\n",
        "    return grad_weight_output, grad_bias_output, grad_weight_hidden, grad_bias_hidden"
      ],
      "metadata": {
        "id": "GxnnhiychjCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating Weight\n",
        "def update_weight_network(weight, cost):\n",
        "    if cost.shape == (features, hidden_layer_nodes) or cost.shape == (hidden_layer_nodes, number_of_classes):\n",
        "        weight = weight - learning_rate * cost\n",
        "    elif cost.shape == (samples, hidden_layer_nodes) or cost.shape == (samples, number_of_classes):\n",
        "        weight = weight - learning_rate * cost.sum(axis=0)\n",
        "    return weight"
      ],
      "metadata": {
        "id": "QJy4WRkEj01_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight initialization\n",
        "#===============================================================================\n",
        "# Hidden Network:\n",
        "hidden_network_weight = np.random.randn(features, hidden_layer_nodes)\n",
        "hidden_network_bias = np.random.randn(hidden_layer_nodes)\n",
        "# Output Network:\n",
        "output_network_weight = np.random.randn(hidden_layer_nodes, number_of_classes)\n",
        "output_network_bias = np.random.randn(number_of_classes)"
      ],
      "metadata": {
        "id": "MCRpVMyjlXWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Table which displays the shapes of the output\n",
        "from tabulate import tabulate\n",
        "table = [\n",
        "    [\"The Shape of X_train dataset\", X_train.shape],\n",
        "    [\"The Shape of hidden_network_weight\", hidden_network_weight.shape],\n",
        "    [\"The Shape of hidden_network_bias\", hidden_network_bias.shape],\n",
        "    [\"The Shape of output_network_weight\", output_network_weight.shape],\n",
        "    [\"The Shape of output_network_bias\", output_network_bias.shape]\n",
        "]\n",
        "headers = [\"Variable\", \"Shape\"]\n",
        "print(tabulate(table, headers, tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RlUpHiZdKgZ",
        "outputId": "41526390-8c9d-40db-befc-ec8cc211820d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+--------------+\n",
            "| Variable                           | Shape        |\n",
            "+====================================+==============+\n",
            "| The Shape of X_train dataset       | (19803, 784) |\n",
            "+------------------------------------+--------------+\n",
            "| The Shape of hidden_network_weight | (784, 20)    |\n",
            "+------------------------------------+--------------+\n",
            "| The Shape of hidden_network_bias   | (20,)        |\n",
            "+------------------------------------+--------------+\n",
            "| The Shape of output_network_weight | (20, 4)      |\n",
            "+------------------------------------+--------------+\n",
            "| The Shape of output_network_bias   | (4,)         |\n",
            "+------------------------------------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of Network Accuracy with the help of Epochs and calculate the Loss Values as well.\n",
        "error_per_epoch = list()\n",
        "epoch = 0\n",
        "for epoch in range(number_of_epoch):\n",
        "    actual_output, actual_hidden, hidden_network = forward_propagation(X_train, hidden_network_weight, hidden_network_bias, output_network_weight, output_network_bias) # Forward propragation\n",
        "    net_wo_cost, net_bo_cost, net_wh_cost, net_bh_cost = back_propagation(X_train, y_train, hidden_network, actual_hidden, output_network_weight, actual_output) # Backward propagation\n",
        "\n",
        "    # Updating the weights and biases of the network\n",
        "    hidden_network_weight = update_weight_network(hidden_network_weight, net_wh_cost)\n",
        "    hidden_network_bias = update_weight_network(hidden_network_bias, net_bh_cost)\n",
        "    output_network_weight = update_weight_network(output_network_weight, net_wo_cost)\n",
        "    output_network_bias = update_weight_network(output_network_bias, net_bo_cost)\n",
        "\n",
        "    # Loss Value Calculation\n",
        "    cal_loss = cross_entropy(y_train,actual_output)\n",
        "    error_per_epoch.append(cal_loss)\n",
        "\n",
        "    # Calculating the prediction accuracy with the trained network\n",
        "    y_pred, _, _ = forward_propagation(X_val,  hidden_network_weight, hidden_network_bias, output_network_weight, output_network_bias)\n",
        "    y_pred_enc = one_hot_encoding(y_pred)\n",
        "    pred_accuracy = accuracy(y_val,y_pred_enc)\n",
        "    if epoch%10==0:\n",
        "        print('epoch = ',epoch,'   ','Loss value: ', cal_loss,'Network accuracy = ',pred_accuracy)"
      ],
      "metadata": {
        "id": "B1aWj3eAoR2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0585bfb-4a2c-4f54-d901-3a94efa28bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch =  0     Loss value:  0.24467540780241673 Network accuracy =  0.2821652191476469\n",
            "epoch =  10     Loss value:  0.10499949268041942 Network accuracy =  0.6691577459099172\n",
            "epoch =  20     Loss value:  0.11806264637882415 Network accuracy =  0.6418905271662291\n",
            "epoch =  30     Loss value:  0.13885949258537295 Network accuracy =  0.6477479297111695\n",
            "epoch =  40     Loss value:  0.048804409479882994 Network accuracy =  0.918400323167037\n",
            "epoch =  50     Loss value:  0.02450510228112919 Network accuracy =  0.9392042011714805\n",
            "epoch =  60     Loss value:  0.020855768176160475 Network accuracy =  0.9432437891335084\n",
            "epoch =  70     Loss value:  0.019530764752499916 Network accuracy =  0.9470813976974348\n",
            "epoch =  80     Loss value:  0.018211058286863667 Network accuracy =  0.9493031710765502\n",
            "epoch =  90     Loss value:  0.016950365447619097 Network accuracy =  0.949707129872753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Caluculation of Validation Set\n",
        "pred_accuracy = accuracy(y_val,y_pred_enc)\n",
        "print('Accuracy for validation set: {}'.format(pred_accuracy*100))"
      ],
      "metadata": {
        "id": "7lzU92zt7Uv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc97762-2218-4f36-ab40-07e10fe49ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for validation set: 95.23328620480711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('hidden_network_weight.npy', hidden_network_weight)\n",
        "np.save('hidden_network_bias.npy', hidden_network_bias)\n",
        "np.save('output_network_weight.npy', output_network_weight)\n",
        "np.save('output_network_bias.npy', output_network_bias)"
      ],
      "metadata": {
        "id": "xjaSZzMI7d4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 4 .npy array format files has been stored to sample_data file of this notebook."
      ],
      "metadata": {
        "id": "m4bXbahHeSgC"
      }
    }
  ]
}